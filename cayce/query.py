"""
Query for available documents directly from EDGAR
"""

import datetime as dt
from os import path, remove
import re
import requests
import shutil
import tempfile
from typing import Union, List, Any
from zipfile import ZipFile

import pandas as pd
import wget

from cayce.utils import (
    ifna,
    split_fixed_length,
    add_months,
    get_quarter,
    get_start_of_quarter,
)
from cayce.log import get_logger


_LOG = get_logger(__name__)


class EdgarIndex:
    _index: pd.DataFrame = None

    # tomorrow will be greater than the maximum allowable date
    _min_date: dt.date = dt.date.today() + dt.timedelta(days=1)
    # epoch is less than the minimum accepted date here
    _max_date: dt.date = dt.datetime.fromtimestamp(0).date()

    def __init__(self, cache_dir: str = None):
        """
        Create a new Edgar filing index

        Args:
            cache_dir (str, optional): 
                Local path where Edgar cache files can be stored. 
                Defaults to None, which will equates to %TEMP%
        """
        if cache_dir:
            self._use_temp = False
            self._cache_dir = cache_dir
            self._index_cache_file = path.join(self._cache_dir, "edgar_filings.idx")
            if path.exists(self._index_cache_file):
                # force everything to be used as a string
                self._index = pd.read_csv(self._index_cache_file).astype(str)
                # except for the filing date, of course...
                self._index["date_filed"] = pd.to_datetime(self._index["date_filed"])
                self._min_date = self._index["date_filed"].min()
                self._max_date = self._index["date_filed"].max()

        else:
            self._use_temp = True
            self._cache_dir = tempfile.mkdtemp()

        if self._index is None:
            self._index = pd.DataFrame(
                [], columns=["company", "form_type", "cik", "date_filed", "file_name"]
            )

    def __del__(self):
        if self._use_temp:
            # Clean up temp directory, if used
            try:
                shutil.rmtree(self._cache_dir)
            except Exception as e:
                _LOG.error(f"Failed to remove temp directory {self._cache_dir}", e)
        else:
            # the latest quarter will get regenerated by EDGAR every day
            # so we should clear out data from the current quarter before saving
            # and delete the current quarter's cached file downlaoded from EDGAR
            current_date = dt.date.today()
            current_quarter_file = path.join(
                self._cache_dir,
                f"{current_date.year}-{get_quarter(current_date)}-index.zip",
            )
            if path.exists(current_quarter_file):
                remove(current_quarter_file)

            current_quarter_start = pd.to_datetime(
                get_start_of_quarter(dt.date.today())
            )
            date_mask = self._index["date_filed"] < current_quarter_start
            self._index[date_mask].to_csv(self._index_cache_file, index=False)

    def _download_index(self, reference_date: dt.date) -> str:
        """
        Download an index file that would encapsulate the specified date

        Args:
            reference_date (dt.date): Reference date that needs to be in the index
        """
        year = str(reference_date.year)
        quarter = get_quarter(reference_date)
        url = (
            f"http://sec.gov/Archives/edgar/full-index/{year}/QTR{quarter}/company.zip"
        )
        local_file_path = path.join(self._cache_dir, f"{year}-{quarter}-index.zip")

        if path.exists(local_file_path):
            _LOG.info(f"Using cached file {local_file_path}")
        else:
            _LOG.info(f"Downloading file {url}")
            wget.download(url, local_file_path)

        return local_file_path

    def _process_company_idx(self, file_name: str) -> pd.DataFrame:
        """
        Process a company.zip file, as retrieved from EDGAR

        Args:
            file_name (str): Local file name

        Returns:
            pd.DataFrame: Content of the index
        """
        # fmt: off
        assert file_name.endswith(".zip"), "Expecting a zipped file containing the company index"
        # fmt: on

        data = []

        with ZipFile(file_name) as zipped:
            assert len(zipped.namelist()) == 1, "Only expecting archive to have 1 file"

            archived_file = zipped.namelist()[0]
            with zipped.open(archived_file) as f:
                header = True
                for line_bytes in f:
                    # skip past all header rows
                    if header:
                        if line_bytes.startswith(b"-------"):
                            header = False
                        continue
                    line = line_bytes.decode("utf-8")
                    data.append(split_fixed_length(line, [62, 12, 12, 12]))

        return pd.DataFrame(
            data, columns=["company", "form_type", "cik", "date_filed", "file_name"]
        )

    def _refresh_index(
        self,
        start_date: dt.date = dt.date(1993, 1, 1),
        end_date: dt.date = dt.date.today(),
    ):
        # fmt: off
        assert start_date >= dt.date(1993, 1, 1), "Sadly, EDGAR's memory only stretches back to Q1 1993"
        assert end_date <= dt.date.today(), "Unfortunately, EDGAR can't see into the future"
        # fmt: on

        # if null, make these values such that we'll never see a sample date fall within the window
        date = start_date
        processed_files = []
        subindex_dfs = []
        while True:
            if date < self._min_date or date > self._max_date:
                file_name = self._download_index(date)

                # since we don't control if the end_date is the first day of a new quarter and the
                # length of the window we search, ensure we don't process the same index twice...
                if file_name not in processed_files:
                    subindex_dfs.append(self._process_company_idx(file_name))
                    processed_files.append(file_name)
            if date == end_date:
                break

            # 3m forward should be in the next quarter
            date = add_months(date, 3)
            if date > end_date:
                # ensure we always query the end date specifically
                date = end_date

        if start_date < self._min_date:
            self._min_date = start_date
        if end_date > self._max_date:
            self._max_date = end_date

        # append all new index files to the master index
        if len(subindex_dfs) > 0:
            subindex_df = pd.concat(subindex_dfs)
            self._index = self._index.append(subindex_df, ignore_index=True)
            self._index["date_filed"] = pd.to_datetime(self._index["date_filed"])

    def search(
        self,
        start_date: dt.date = dt.date(1993, 1, 1),
        end_date: dt.date = dt.date.today(),
        ciks: Union[str, List[str]] = None,
        form_types: Union[str, List[str]] = None,
    ):
        """
        Search through the EDGAR company indices to find filings matching a provided criteria

        Args:
            start_date (dt.date, optional): Earliest date to accept. Defaults to 1993-01-01 (the earliest date on EDGAR).
            end_date (dt.date, optional): Latest date to accept. Defaults to today() (implicitly the latest possible date).
            ciks (Union[str, List[str]], optional):
                Provide one or more CIK values to filter on. Single value can be passed a string, multiple as a list.  
                Defaults to None, which doesn't filter on this column.
            form_types (Union[str, List[str]], optional):
                Provide one or more form types to filter on. Single value can be passed a string, multiple as a list.  
                Defaults to None, which doesn't filter on this column.
        """
        self._refresh_index(start_date, end_date)

        if ciks:
            if isinstance(ciks, str):
                ciks = [ciks]
            result_df = self._index[self._index["cik"].isin(ciks)]
        else:
            result_df = self._index

        result_df = result_df[
            (result_df["date_filed"] >= pd.to_datetime(start_date))
            & (result_df["date_filed"] <= pd.to_datetime(end_date))
        ]

        if form_types:
            if isinstance(form_types, str):
                form_types = [form_types]
            result_df = result_df[result_df["form_type"].isin(form_types)]

        return result_df

    def download_xbrl(self, search_record: List[Any], save_raw: bool = False) -> str:
        """
        Pull a full filing from the SEC website and strip out everything
        outside of the XBRL content for this specific form

        Args:
            search_record: 
                The value array of a row taken from the `search` method
                Expected to be the following elements, in order:
                    Company Name
                    Form Type
                    CIK code
                    Filing Date
                    File Name (partial URL from edgar)
            save_raw: Do we save the full archive file from EDGAR?
        
        Returns:
            (str) Full path to the local file
        """
        company, form_type, _, date_filed, file_name = search_record
        _LOG.info(
            f"Begin downloading {company} form {form_type} for {date_filed:%Y-%m-%d}"
        )
        url = f"https://www.sec.gov/Archives/{file_name}"

        response = requests.get(url)
        file_content = response.content.decode("utf-8").split("\n")

        cleaned_company_name = re.sub("\W+", "_", company)
        file_suffix = file_name.split("/")[-1].split(".")[0].split("-")[-1]

        if save_raw:
            with open(
                path.join(
                    self._cache_dir,
                    "raw",
                    f"{cleaned_company_name}_{form_type}_{date_filed:%Y%m%d}_{file_suffix}.txt",
                ),
                mode="w",
            ) as raw_file:
                raw_file.write("\n".join(file_content))

        if form_type == "10-K" or form_type == "10-Q":
            document_payload = self._get_financial_statement_payload(file_content)
        elif form_type == "4":
            document_payload = self._get_beneficial_ownership_payload(file_content)
        else:
            raise ValueError(f"Content parser not available for {form_type}")

        local_xbrl_file_path = path.join(
            self._cache_dir,
            "xbrl",
            f"{cleaned_company_name}_{form_type}_{date_filed:%Y%m%d}_{file_suffix}.xml",
        )
        _LOG.info(f"Writing local cache file {local_xbrl_file_path}")
        with open(local_xbrl_file_path, mode="w") as xbrl_writer:
            xbrl_writer.writelines(document_payload)

        return local_xbrl_file_path

    def _get_financial_statement_payload(self, file_content: List[str]):
        """
        Extract filing payload for 10-Q and 10-K filings
        """
        found_document = False
        found_xbrl_payload = False
        xbrl_payload = []
        found_document_re = re.compile(
            "^(<DESCRIPTION>(XBRL INSTANCE (DOCUMENT|FILE)|EX-101.INS)|<TYPE>EX-101.INS|<FILENAME>.+_htm\.xml)$",
            re.IGNORECASE,
        )
        end_payload_tag = (
            "<>/\[]/\<>"  # nonsense line, just a placeholder, should never be found
        )
        for line in file_content:
            if not found_document:
                if re.match(found_document_re, line):
                    found_document = True
                continue
            elif not found_xbrl_payload:
                if line.lower().strip() == "<xml>":
                    found_xbrl_payload = True
                    end_payload_tag = "</xml>"
                elif line.lower().strip() == "<xbrl>":
                    found_xbrl_payload = True
                    end_payload_tag = "</xbrl>"
                continue
            elif line.lower().strip() == end_payload_tag:
                break
            else:
                xbrl_payload.append(line)
        return xbrl_payload

    def _get_beneficial_ownership_payload(self, file_content: List[str]):
        """
        Extract filing payload for Form 4 filings
        """
        found_document = False
        found_payload = False
        xml_payload = []
        found_document_re = re.compile(
            "^<DESCRIPTION>(FORM 4|PRIMARY DOCUMENT)$", re.IGNORECASE
        )
        found_payload_re = re.compile("^<xml>", re.IGNORECASE)
        end_payload_re = re.compile("</xml>", re.IGNORECASE)
        for line in file_content:
            if not found_document:
                if re.match(found_document_re, line):
                    found_document = True
                continue
            elif not found_payload:
                if re.match(found_payload_re, line):
                    found_payload = True
                continue
            elif re.match(end_payload_re, line):
                break
            else:
                xml_payload.append(line)
        return xml_payload
